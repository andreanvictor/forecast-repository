{% extends "base.html" %}

{% block title %}About{% endblock %}

{% block content %}


    <h1>About the Forecast Repository</h1>

    <p>Welcome to <em>Forecast Repository</em>, a Django website to prototype our ideas at
        <a href="http://reichlab.io/">Nicholas Reich's lab</a> to create a repository of model forecast results along
        with tools to compare, etc.
    </p>


    <h2>Introduction</h2>

    <p>Until now, predictions made by models in the lab have been stored in different formats and in different
        locations. This makes it difficult to track, compare, and revisit forecasts. This document outlines this system,
        which supports storing and retrieving time series forecasts for prediction challenges of interest to us. The
        goal is to be able to save and retrieve model information and forecast data for use in applications like
        visualization, calculating loss/error, and comparing models.</p>


    <h2>Terminology</h2>

    {% include 'terminology_snippet.html' %}


    <h2>Assumptions/Limitations</h2>

    <p>The scope of this first iteration is limited in these ways:</p>

    <ul>
        <li><u>Process-agnostic</u>: By storing only core datasets, we make no assumptions about ML processes behind a
            model’s forecast, such as how it’s fit.
        </li>
        <li><u>Enforceability</u>: There is currently not a method in place to test whether the models were fit on the
            right data subsets (this is something that the below ForecastFramework integration could help with).
        </li>
        <li><u>Unrevised vs. revised data</u>: A Project’s core dataset may or may not include data revisions, such as
            those used to model or forecast reporting delays. Each project should give specific instructions on what
            type of data (revised vs. unrevised) is used in the training and testing phases of the forecasting.
        </li>
        <li><u>Model instances</u>: The system stores only model metadata, rather than computable representations of
            models (internals) that could be used to reconstruct and re-run them.
        </li>
        <li><u>Reports</u>: Some projects generate automated narrative reports from forecast data. This system does not
            support storing reports with their models.
        </li>
        <li><u>Training/testing data</u>: The only information about what subsets of the core data were used for
            different ML stages (e.g. training vs testing) will be stored in narrative format in the project
            description.
        </li>
        <li><u>Reproducibility</u>: Since this system stores data involved in forecasts and not source code, information
            about how to re-run models is only captured in narrative form in the model's description, and is linked to
            by the model's url field.
        </li>
        <li><u>Metrics</u>: This version does not capture metric information. If metrics change, then a new project
            should be created.
        </li>
    </ul>


    <h2>Funding</h2>
    <p>This work has been supported by the National Institutes of General Medical Sciences (R35GM119582). The content
        is solely the responsibility of the authors and does not necessarily represent the official views of NIGMS, or
        the National Institutes of Health.
    </p>


    <h2>Contact</h2>

    <p>If you have questions about this site or want an account, please contact Professor Nicholas Reich
        (nick@schoolph.umass.edu>), director of the <a href="http://reichlab.io/">Reich Lab</a>.
    </p>

{% endblock %}
